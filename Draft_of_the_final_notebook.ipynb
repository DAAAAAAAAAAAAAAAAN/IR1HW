{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Hypothesis Testing - the problem of multiple comparisons (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type I error occurs in situations where the null hypothesis is rejected while $H_0$ is true and must not be rejected. Thus, the probability of making an incorrect decision on the statisitical significance for a given experiment is $P ($type I error$) = alpha$. Conversely, the probability of the correct rejection of the null hypothesis is $P ($correct $H_0$ rejection$) = 1 - alpha$. <br><br>\n",
    "_Main question w.r.t. (a): are we supposed to estimate the prob of (one?) m-th significant result or (one?) m-th FALSE significant result as follows from the plot in slide 35, lecture 2b?_ <br>\n",
    "(a) <br>Interpretation 1:<br>\n",
    "$P ($not making a type I error in $(m - 1)$ experiments but making it in the m-th experiment alone$) = (1 - alpha)^{m-1}*alpha$<br>\n",
    "Interpretation 2:<br>\n",
    "$P ($not making a type I error in $m$ experiments$) = (1 - alpha)^m$<br>\n",
    "\n",
    "(b) For a series of $m$ experiments, we can estimate the probability of having performed at least one significant experiment as the inverse probability of making Type I error in all $m$ tests. Thus, <br>\n",
    "Interpretation 1:<br>\n",
    "$P ($type I error in one experiment$) = alpha$<br>\n",
    "$P ($type I error in all $m$ experiments$) = alpha^m$<br>\n",
    "$P ($at least one significant experiment$) = 1 - alpha^m$<br>\n",
    "Interpretation 2 (one FALSE significant result):<br>\n",
    "$P ($type I error in one experiment$) = alpha$<br>\n",
    "$P ($not making a type I error in one experiment$) = 1 - alpha$<br>\n",
    "$P ($not making a type I error in all $m$ experiments$) = {(1 - alpha)}^m$<br>\n",
    "$P ($at least one FALSE significant experiment in $m$ experiments$) = 1 - {(1 - alpha)}^m$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Bias and unfairness in Interleaving experiments (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team-draft interleaving method makes an incorrect judgement when, for instance, there exists only one relevant (clicked) document and it is ranked differently by two different algorithms. <br>\n",
    "Let us consider the following example. Given two ranking algorithms A and B, we assume that these algorithms operate on a collection of three documents ($d_1$, $d_2$, $d_3$) where only one of them (say, $d_3$, is relevant). Hence, algorithm A ranks the documents in the following order: $d_1: N$, $d_2:N$, $d_3:R$ while algorithm B ranks them as follows: $d_2: N$, $d_3:R$, $d_1:N$. Intuitively, algorithm B must win the competition as it assigns a higher rank to the only relevant document. However, this does not take place in reality if the team-draft interleaving method is applied. <br>\n",
    "As the order of picking the documents is randomized, there exist four possible assignments (\"ABA\", \"ABB\", \"BAA\", \"BAB\"), which represent the order in which each algorithm (encoded as a literal in a given sequence) picks a preferred document. The resulting rankings are as follows: <br><br>\n",
    "**ABA**: \n",
    "> {$d_1: N$} (from A), {$d_2: N$} (from B), {$d_3: R$} (from A)\n",
    "\n",
    "**ABB**: \n",
    "> {$d_1: N$} (from A), {$d_2: N$} (from B), {$d_3: R$} (from B)\n",
    "\n",
    "**BAA**: \n",
    "> {$d_2: N$} (from B), {$d_1: N$} (from A), {$d_3: R$} (from A)\n",
    "\n",
    "**BAB**: \n",
    "> {$d_2: N$} (from B), {$d_1: N$} (from A), {$d_3: R$} (from B)\n",
    "\n",
    "We can observe that the only relevant document $d_3$ is ranked to appear third in all the assignments. As a result, the team-draft method considers this situation a tie while it is clear from the initial conditions that algorithm B should be preferred over algorithm A. In conclusion, the case described above represents a situation where the team-draft interleaving method incorrectly evaluates given ranking algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Rankings of Relevance for _E_ and _P_ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given three relevance labels (N, R, and HR), we assume the following relevance scores. Thus, non-relevant documents have a zero score, relevant documents have a score of 1, highly relevant documents are scored 2. Assuming ranking pairs to be of length 5, we then generate $3^5 = 243$ possible rankings for production algorithm P and experimental algorithm E.<br>We can represent the given rankings as an $(n*n)$-matrix, where $n$ is the number of possible rankings for P and E. Given this representation, we can conclude that there must exist $243 * 243 = 59049$ possible ranking pairs of P and E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58806 unique pairs of rankings\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "import numpy\n",
    "import random\n",
    "import math\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "relevances = { 'N', 'R', 'HR' }\n",
    "relevanceScores = { 'N':0, 'R':1, 'HR':5 }\n",
    "rankingsOf5 = list(itertools.product(relevances, repeat=5))\n",
    "pairsOfRankingsOf5 = list(itertools.product(rankingsOf5, rankingsOf5))\n",
    "# remove irrelevant rankings that cause divide by zero errors\n",
    "pairsOfRankingsOf5.remove((('N','N','N','N','N'),('N','N','N','N','N')))\n",
    "# remove pairs of rankings that are the same\n",
    "pairsOfRankingsOf5 = list(filter(lambda pair: pair[0] != pair[1] , pairsOfRankingsOf5))\n",
    "shuffle(pairsOfRankingsOf5)\n",
    "print(len(pairsOfRankingsOf5), 'unique pairs of rankings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Evaluation Measures (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the evaluation metrics, we have selected to implement average precision, normalised discounted cumulative gain at rank _k_ (nDCG@k), and expected reciprocal rank (ERR). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getContingencies (items, k, relevantDocumentCount):\n",
    "    retrievedCounter = Counter(items[:k])\n",
    "    TP = retrievedCounter['R'] + retrievedCounter['HR']\n",
    "    FP = retrievedCounter['N']\n",
    "    \n",
    "    notRetrievedCounter = Counter(items[k:])\n",
    "    TN = notRetrievedCounter['N']\n",
    "    FN = relevantDocumentCount - TP\n",
    "    \n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def getPrecisionAtK (ranking, k):\n",
    "    TP, FP, TN, FN = getContingencies(ranking, k, relevantDocumentCount)\n",
    "    precisionAtK = TP / (TP + FP)\n",
    "#     recallAtK = TP / (TP + FN)\n",
    "#     F1AtK = 2*precisionAtK*recallAtK\n",
    "#     if F1AtK > 0.0: \n",
    "#         F1AtK /= precisionAtK + recallAtK\n",
    "#     accuracyAtK = (TP + TN)/(TP + FP + FN + TN)\n",
    "    return precisionAtK\n",
    "\n",
    "def getAveragePrecision (ranking, relevantDocumentCount):\n",
    "    precisionsForAp = []\n",
    "    for k in range(1, len(ranking)+1):\n",
    "        precisionAtK = getPrecisionAtK(ranking, k)\n",
    "\n",
    "        if ranking[k-1] == 'R' or ranking[k-1] == 'HR':\n",
    "            # save for calculating AP later\n",
    "            precisionsForAp.append(precisionAtK)\n",
    "    \n",
    "    averagePrecision = sum(precisionsForAp)/relevantDocumentCount\n",
    "    return averagePrecision\n",
    "\n",
    "def getDiscountedCumulativeGain (ranking):\n",
    "    dcg = 0.0\n",
    "    for r in range(1, len(ranking)+1):\n",
    "        relevanceAtR = relevanceScores[ranking[r-1]]\n",
    "        gain = (2 ** relevanceAtR) - 1\n",
    "        discount = math.log2(1 + r)\n",
    "        dcg += gain/discount\n",
    "    return dcg\n",
    "\n",
    "def getProbabilityOfRelevance (relevance):\n",
    "    # from paper, similar to DCG score\n",
    "    gain = (2 ** relevanceScores[relevance]) - 1\n",
    "    discount = 2 ** max(relevanceScores.values())\n",
    "    probability = gain / discount\n",
    "#     print('probability of', relevance, 'is', probability)\n",
    "    return probability\n",
    "\n",
    "def getExpectedReciprocalRank (ranking):\n",
    "    err = 0.0\n",
    "    for r in range(1, len(ranking)+1):\n",
    "        probabilityOfReachingRankR = 1.0\n",
    "        for j in range(r-1):\n",
    "            probabilityOfReachingRankR *= 1 - getProbabilityOfRelevance(ranking[j])\n",
    "        probabilityOfStoppingAtRankR = getProbabilityOfRelevance(ranking[r-1])\n",
    "        probabilityOfSatisfaction = probabilityOfReachingRankR * probabilityOfStoppingAtRankR\n",
    "        expectedProbabilityOfSatisfaction = probabilityOfSatisfaction / r\n",
    "        err += expectedProbabilityOfSatisfaction\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate $\\delta$-measure (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the next step, we count the number of pairs where the experimental algorithm outperforms the production algorithm and calculate the difference in performance with respect to all the three pre-selected evaluation metrics for such pairs (we ignore one ranking pair where all the P and E's documents are irrelevant). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P:  ('R', 'N', 'HR', 'N', 'N') \n",
      "E:  ('N', 'HR', 'HR', 'HR', 'N')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'R', 'N', 'N', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 66.05882236071518\n",
      "AP: \t\tP:0.333 \tE:0.383\n",
      "nDCG@k=3: \tP:0.250 \tE:0.531\n",
      "ERR: \t\tP:0.344076 \tE:0.494703\n",
      "\n",
      "P:  ('N', 'HR', 'HR', 'HR', 'HR') \n",
      "E:  ('N', 'R', 'HR', 'N', 'R')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'HR', 'R', 'R', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 66.05882236071518\n",
      "AP: \t\tP:0.388 \tE:0.252\n",
      "nDCG@k=3: \tP:0.531 \tE:0.244\n",
      "ERR: \t\tP:0.494709 \tE:0.328640\n",
      "\n",
      "P:  ('N', 'HR', 'HR', 'N', 'HR') \n",
      "E:  ('R', 'HR', 'R', 'N', 'R')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'R', 'R', 'R', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 66.05882236071518\n",
      "AP: \t\tP:0.252 \tE:0.543\n",
      "nDCG@k=3: \tP:0.531 \tE:0.319\n",
      "ERR: \t\tP:0.494655 \tE:0.500987\n",
      "\n",
      "P:  ('HR', 'R', 'R', 'HR', 'HR') \n",
      "E:  ('HR', 'N', 'N', 'HR', 'HR')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'HR', 'HR', 'R', 'R', 'N', 'N']\n",
      "perfect DCG score:\t 66.05882236071518\n",
      "AP: \t\tP:0.625 \tE:0.263\n",
      "nDCG@k=3: \tP:0.486 \tE:0.469\n",
      "ERR: \t\tP:0.976834 \tE:0.976508\n",
      "\n",
      "P:  ('N', 'HR', 'HR', 'HR', 'N') \n",
      "E:  ('HR', 'N', 'R', 'R', 'HR')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'HR', 'R', 'R', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 66.05882236071518\n",
      "AP: \t\tP:0.274 \tE:0.460\n",
      "nDCG@k=3: \tP:0.531 \tE:0.477\n",
      "ERR: \t\tP:0.494703 \tE:0.974994\n",
      "\n",
      "\n",
      "Out of 59048 rankings, E outperformed P:\n",
      "AP: \t\t47.340%\n",
      "nDCG@k=3: \t48.148%\n",
      "ERR: \t\t49.767%\n",
      "MAP \t\tP:0.399 \tE:0.399\n"
     ]
    }
   ],
   "source": [
    "averagePrecisionsForMapP = []\n",
    "averagePrecisionsForMapE = []\n",
    "pairCountForWhichEHasBetterAp = 0\n",
    "pairCountForWhichEHasBetterNDcgAtK = 0\n",
    "pairCountForWhichEHasBetterErr = 0\n",
    "for i, rankingPair in enumerate(pairsOfRankingsOf5):\n",
    "    P = rankingPair[0]\n",
    "    E = rankingPair[1]\n",
    "\n",
    "    totalCounter = Counter(P) + Counter(E)\n",
    "    relevantDocumentCount = totalCounter['R'] + totalCounter['HR']\n",
    "    \n",
    "    # implement 1 of (binary):\n",
    "    #   precision at rank k\n",
    "    #   recall at rank k\n",
    "    #   average precision   <--\n",
    "    averagePrecisionP = getAveragePrecision(P, relevantDocumentCount)\n",
    "    averagePrecisionE = getAveragePrecision(E, relevantDocumentCount)\n",
    "    \n",
    "    # save for calculating MAP later\n",
    "    averagePrecisionsForMapP.append(averagePrecisionP)\n",
    "    averagePrecisionsForMapE.append(averagePrecisionE)\n",
    "    \n",
    "    # Normalized Discounted Cumulative Gain\n",
    "    # First we have to determine the perfect ranking. Assuming the P and E results are always\n",
    "    # different, and that both algorithms run on the same corpus of documents, the perfect ranking \n",
    "    # would include the results from both rankings.\n",
    "    k = 3\n",
    "    mergedRanking = P + E\n",
    "    perfectRanking = sorted(mergedRanking, key=lambda relevance: relevanceScores[relevance], reverse=True)\n",
    "    perfectDcgScore = getDiscountedCumulativeGain(perfectRanking[:k])\n",
    "    dcgAtKP = getDiscountedCumulativeGain(P[:k])\n",
    "    dcgAtKE = getDiscountedCumulativeGain(E[:k])\n",
    "    nDcgAtKP = dcgAtKP / perfectDcgScore\n",
    "    nDcgAtKE = dcgAtKE / perfectDcgScore\n",
    "    \n",
    "    \n",
    "    # Expected Reciprocal Rank\n",
    "    errP = getExpectedReciprocalRank(P)\n",
    "    errE = getExpectedReciprocalRank(E)\n",
    "    \n",
    "    # calculate delta measures\n",
    "    deltaAp = averagePrecisionE - averagePrecisionP\n",
    "    deltaNDcgAtK = nDcgAtKE - nDcgAtKP\n",
    "    deltaErr = errE - errP\n",
    "    \n",
    "    # count pairs for which E outperforms P\n",
    "    epsilon = 1e-6 # avoid floating point imprecisions\n",
    "    if deltaAp > epsilon:\n",
    "        pairCountForWhichEHasBetterAp += 1\n",
    "    if deltaNDcgAtK > epsilon:\n",
    "        pairCountForWhichEHasBetterNDcgAtK += 1\n",
    "    if deltaErr > epsilon:\n",
    "        pairCountForWhichEHasBetterErr += 1\n",
    "    \n",
    "    # only show a few\n",
    "    if i < 5:\n",
    "        # show the pair\n",
    "        print ('\\nP: ', P, '\\nE: ', E)\n",
    "        print('perfect ranking:\\t', perfectRanking)\n",
    "        print('perfect DCG score:\\t', perfectDcgScore)\n",
    "        print('AP: \\t\\tP:{:.3f} \\tE:{:.3f}'.format(averagePrecisionP, averagePrecisionE))\n",
    "        print('nDCG@k={}: \\tP:{:.3f} \\tE:{:.3f}'.format(k, nDcgAtKP, nDcgAtKE))\n",
    "        print('ERR: \\t\\tP:{:.6f} \\tE:{:.6f}'.format(errP, errE))\n",
    "\n",
    "\n",
    "# print results\n",
    "\n",
    "# print how many times E outperformed P\n",
    "print('\\n\\nOut of {} rankings, E outperformed P:'.format(len(pairsOfRankingsOf5)-1))\n",
    "print('AP: \\t\\t{:.3%}'.format(pairCountForWhichEHasBetterAp/len(pairsOfRankingsOf5)))\n",
    "print('nDCG@k={}: \\t{:.3%}'.format(k, pairCountForWhichEHasBetterNDcgAtK/len(pairsOfRankingsOf5)))\n",
    "print('ERR: \\t\\t{:.3%}'.format(pairCountForWhichEHasBetterErr/len(pairsOfRankingsOf5)))\n",
    "        \n",
    "# we accidentally implemented MAP instead of just AP, but we'll leave it in\n",
    "meanAveragePrecisionP = sum(averagePrecisionsForMapP)/len(averagePrecisionsForMapP)\n",
    "meanAveragePrecisionE = sum(averagePrecisionsForMapE)/len(averagePrecisionsForMapE)\n",
    "print('MAP \\t\\tP:{:.3f} \\tE:{:.3f}'.format(meanAveragePrecisionP, meanAveragePrecisionE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated the $\\delta$-measures, we proceed by implementing the balanced interleaving method and the team-draft interleaving method for online evaluation of the ranking algorithms. As P and E are assumed to always return different documents, we additionally index each document considered within a ranking pair to distinguish them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking pair:  [['HR', 'HR', 'R', 'R', 'N'], ['HR', 'R', 'R', 'N', 'N']]\n",
      "Updated pair:  (['HR0', 'HR1', 'R2', 'R3', 'N4'], ['HR5', 'R6', 'R7', 'N8', 'N9'])\n",
      "interleaved:  [('HR0', 0), ('HR5', 1), ('R6', 1), ('HR1', 0), ('R2', 0), ('R7', 1), ('R3', 0), ('N8', 1), ('N9', 1), ('N4', 0)]\n",
      "Random Clicks:  [4, 6, 7]\n",
      "Evaluation A-B (number of clicks):  1\n"
     ]
    }
   ],
   "source": [
    "###### STEP 4 ######\n",
    "\n",
    "#Team Draft Interleaving\n",
    "\n",
    "\n",
    "#to be able to distinguish the document's assignment, rename the relevances by adding index number\n",
    "#eg. Ranking pair:  [['HR', 'HR', 'R', 'R', 'N'], ['HR', 'R', 'R', 'N', 'N']]\n",
    "# Updated pair : (['HR0', 'HR1', 'R2', 'R3', 'N4'], ['HR5', 'R6', 'R7', 'N8', 'N9'])\n",
    "\n",
    "def preprocess(pair):\n",
    "    i = 0\n",
    "    updated_pair = ([], [])\n",
    "    for relevance in pair[0]:\n",
    "        updated_pair[0].append(relevance+ str(i))\n",
    "        i+=1\n",
    "    \n",
    "    for relevance in pair[1]:\n",
    "        updated_pair[1].append(relevance+ str(i))\n",
    "        i+=1        \n",
    "    \n",
    "    return updated_pair\n",
    "\n",
    "\n",
    "#a) create the interleaving list\n",
    "def team_draft_interleave(pair):\n",
    "    pair = preprocess(pair)\n",
    "    \n",
    "    A = pair[0]\n",
    "    B = pair[1]\n",
    "    interleaved = [] #the interleaving list\n",
    "    assignments = [] #holds the pair that the documents taken from, \"A or B\", \"0 or 1\" respectively\n",
    "    for i in range(len(A)): # only works for rankings of equal lengths without duplicates\n",
    "        if random.random() >= 0.5:\n",
    "            interleaved.append(A[i])\n",
    "            interleaved.append(B[i])\n",
    "            assignments.append(0)\n",
    "            assignments.append(1)\n",
    "        else:\n",
    "            interleaved.append(B[i])\n",
    "            interleaved.append(A[i])\n",
    "            assignments.append(1)\n",
    "            assignments.append(0)\n",
    "#     while A or B:\n",
    "#         if A and B:\n",
    "#             first = random.randint(0,1)\n",
    "#         elif A:\n",
    "#             first = 0\n",
    "#         else:\n",
    "#             first = 1\n",
    "#         second = 1 - first\n",
    "\n",
    "#         #pick from the first list(A) then the second list (B)\n",
    "#         if first == 0: #A\n",
    "#             doc = A[0]\n",
    "#             doc_list.append(doc)\n",
    "#             assignments.append(first)\n",
    "#             #delete the doc from the first and the second list\n",
    "#             del A[0] \n",
    "#             if doc in B:\n",
    "#                 B.remove(doc)\n",
    "            \n",
    "#             #pick from the second list\n",
    "#             doc = B[0]\n",
    "#             doc_list.append(doc)\n",
    "#             assignments.append(second) \n",
    "#             #delete the doc from the second list (top element) and the first list\n",
    "#             del B[0]\n",
    "            \n",
    "#             if doc in A:\n",
    "#                 A.remove(doc)\n",
    "            \n",
    "#        #pick from the first list(B) then the second list (A)\n",
    "#         else: #B\n",
    "#             doc = B[0]\n",
    "#             doc_list.append(doc)\n",
    "#             assignments.append(first)\n",
    "#             del B[0]\n",
    "#             if doc in A:\n",
    "#                 A.remove(doc)\n",
    "\n",
    "#             #pick from the second list\n",
    "#             doc = A[0]\n",
    "#             doc_list.append(doc)\n",
    "#             assignments.append(second)\n",
    "#             #delete the doc from the first and the second list\n",
    "#             del A[0] \n",
    "#             if doc in B:\n",
    "#                 B.remove(doc)\n",
    "    \n",
    "       \n",
    "    #append the interleaved list with the assignment        \n",
    "    output = list(zip(interleaved, assignments))\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "#generate a list of 'number' amount of random clicks\n",
    "def random_clicks(length, number):\n",
    "    clicks = random.sample(range(length), number)\n",
    "    return clicks\n",
    "\n",
    "    \n",
    "#b) evaluate the interleaved list with random clicks\n",
    "\n",
    "\n",
    "def team_draft_evaluation(interleaved, clicks):\n",
    "    doc_list, assignments = zip(*interleaved) \n",
    "#     print(doc_list)\n",
    "#     print(assignments)\n",
    "    click_nums = [0, 0]\n",
    "    for i in clicks:\n",
    "        click_nums[assignments[i]]+=1\n",
    "\n",
    "    return click_nums[0]-click_nums[1]\n",
    "\n",
    "\n",
    "pair = [[\"HR\", \"HR\", \"R\", \"R\", \"N\"], [\"HR\", \"R\", \"R\", \"N\", \"N\"]]\n",
    "print (\"Ranking pair: \", pair)\n",
    "updated_pair = preprocess(pair)\n",
    "print(\"Updated pair: \", updated_pair)\n",
    "interleaved = team_draft_interleave(pair)\n",
    "print(\"interleaved: \", interleaved)\n",
    "\n",
    "clicks = random_clicks(len(interleaved), 3)\n",
    "print(\"Random Clicks: \", clicks)\n",
    "\n",
    "result = team_draft_evaluation(interleaved, clicks)\n",
    "print(\"Evaluation A-B (number of clicks): \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement User Clicks Simulation (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiment, we consider two click models: Random Click Model (RCM) and Simple Dependent Click Model (SDCM). Having browsed the Yandex Click Log File, we observed that there existed some relevant documents had been clicked multiple times. As the selected click models do not take into account multiple clicks for one document, we only consider the first click on such relevant documents and skip the rest.<br>Attractiveness is estimated as the probability of relevance (__refer to the paper here__). Satisfaction is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12631295132701867\n",
      "[0, 6]\n"
     ]
    }
   ],
   "source": [
    "### STEP 5 ####\n",
    "\n",
    "# Yandex Click Log File\n",
    "def get_sessions():\n",
    "    f = open('YandexRelPredChallenge.txt', 'r')\n",
    "    content = []\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        content.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #print(content[:10])\n",
    "\n",
    "    #each session is a query that has the list of documents retrieved and list of documents that are clicked\n",
    "    sessions = []  \n",
    "    for line in content:\n",
    "        if(line[2]) == 'Q': \n",
    "            session = []\n",
    "            session.append(line[5:])\n",
    "            session.append([])\n",
    "            sessions.append(session)\n",
    "        if(line[2]) == 'C':\n",
    "            c = line[-1] \n",
    "            for i in range(-1, -len(sessions), -1): #attribute the click to the last query that had this document as a result\n",
    "                session = sessions[i]\n",
    "                if c in session[0]:\n",
    "                    sessions[i][1].append(c)\n",
    "                    break;\n",
    "    return sessions\n",
    "\n",
    "sessions = get_sessions()\n",
    "\n",
    "#Random Click Model\n",
    "\n",
    "\n",
    "#parameter from the user log\n",
    "def RCM_parameter(sessions):\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    for s in sessions:\n",
    "        clicks = set()\n",
    "        for c in s[1]:\n",
    "            clicks.add(c)\n",
    "        sum1 += len(clicks)\n",
    "        sum2 += len(s[0])\n",
    "        \n",
    "    p = 1.0*sum1/sum2 \n",
    "        \n",
    "    return p\n",
    "\n",
    "\n",
    "def RCM_clicks(length, p):\n",
    "    clicks = []\n",
    "    for i in range(length):\n",
    "        prob = random.uniform(0, 1)\n",
    "        if prob < p:\n",
    "            clicks.append(i)\n",
    "    return clicks\n",
    "\n",
    "p = RCM_parameter(sessions)\n",
    "print(p)\n",
    "\n",
    "clicks = RCM_clicks(10, p)\n",
    "print(clicks)\n",
    "\n",
    "interleaved = [['HR0', 0], ['HR5', 1], ['R6', 1], ['HR1', 0], ['R7', 1], ['R2', 0], ['N8', 1], ['R3', 0], ['N9', 1], ['N4', 0]]\n",
    "\n",
    "def getProbabilityOfRelevance (relevance):\n",
    "    # from paper, similar to DCG score\n",
    "    gain = (2 ** relevanceScores[relevance]) - 1\n",
    "    discount = 2 ** max(relevanceScores.values())\n",
    "    probability = gain / discount\n",
    "    return probability\n",
    "\n",
    "def attractiveness(interleaved):\n",
    "    attr = []\n",
    "    for doc in interleaved:\n",
    "        relevance = doc[0][0]\n",
    "        if relevance == 'H':\n",
    "            attr.append(getProbabilityOfRelevance (\"HR\"))\n",
    "        if relevance == 'R':\n",
    "            attr.append(getProbabilityOfRelevance (\"R\"))\n",
    "        if relevance == 'N':\n",
    "            attr.append(getProbabilityOfRelevance (\"N\"))\n",
    "    return attr\n",
    "    \n",
    "\n",
    "\n",
    "def lambda_r(sessions, r):\n",
    "    S = list(filter(lambda s: s[0][r-1] in s[1] , sessions))\n",
    "    total = 0\n",
    "    for s in S:\n",
    "        click_index = [s[0].index(page) for page in s[1]]\n",
    "        last_click = click_index[-1] + 1 #satisfied\n",
    "        if last_click != r:\n",
    "            total += 1\n",
    "\n",
    "    return total/float(len(S))  \n",
    "    \n",
    "\n",
    "def get_lambdas(sessions):\n",
    "    lambdas = []\n",
    "    for i in range(10):\n",
    "        lambdas.append(lambda_r(sessions, i+1))\n",
    "    return lambdas\n",
    "    \n",
    "\n",
    "def SDCM_clicks(interleaved, lambdas):\n",
    "    clicks = []\n",
    "    attr = attractiveness(interleaved)\n",
    "    n = len(attr)\n",
    "    for i in range(1, n):\n",
    "        prob = random.uniform(0,1)\n",
    "        \n",
    "        if prob < attr[i]: #clicked\n",
    "            clicks.append(i)\n",
    "            prob = random.uniform(0,1)\n",
    "            if prob > lambdas[i]: #stop\n",
    "                break\n",
    "                \n",
    "    return clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Simulate Interleaving Experiment (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Justification for the code in the section below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambdas:[0.31942931258106355, 0.5378201310303752, 0.5706431189603466, 0.58397365532382, 0.5779661016949152, 0.5579487179487179, 0.5545207605743112, 0.5058619192357794, 0.468378506894912, 0.22727272727272727]\n",
      "31 33\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "###### STEP 6 #####\n",
    "\n",
    "# For simulation, we need to run the following piece of code for each pair to get the score\n",
    "# result : which search engine has higher clicks, A - B, eg. if it is higher than 0, the first one wins\n",
    "# each run has a different result, so \n",
    "\n",
    "pair = [[\"HR\", \"HR\", \"R\", \"R\", \"N\"], [\"HR\", \"R\", \"R\", \"N\", \"N\"]]\n",
    "sessions = get_sessions()   #gets the sessions as a list from yandex log file\n",
    "p = RCM_parameter(sessions) #calculates the RCM parameter, MLE\n",
    "lambdas = get_lambdas(sessions)  #calculates all the lambda values for each rank (1-10)\n",
    "print(\"lambdas:{}\".format(lambdas))\n",
    "\n",
    "# timerCount = 0\n",
    "# totalTimeInterleaving = 0.0\n",
    "# totalTimeClicking = 0.0\n",
    "# totalTimeEvaluating = 0.0\n",
    "def simulation_for_pair(pair, click_model):\n",
    "#     global timerCount, totalTimeInterleaving, totalTimeClicking, totalTimeEvaluating\n",
    "#     timerCount += 1\n",
    "#     timer1 = timer()\n",
    "    interleaved = team_draft_interleave(pair)  #gets the interleaved list\n",
    "#     totalTimeInterleaving += timer() - timer1\n",
    "    \n",
    "    if click_model == \"RCM\":\n",
    "#         timer1 = timer()\n",
    "        clicks_RCM = RCM_clicks(10, p)  #generates clicks based on RCM model\n",
    "#         totalTimeClicking += timer() - timer1\n",
    "#         timer1 = timer()\n",
    "        result = team_draft_evaluation(interleaved, clicks_RCM)\n",
    "#         totalTimeEvaluating += timer() - timer1\n",
    "    if click_model == \"SDCM\":\n",
    "        clicks_SDCM = SDCM_clicks(interleaved, lambdas) #generates clicks based on RCM model\n",
    "        result = team_draft_evaluation(interleaved, clicks_SDCM)\n",
    "    \n",
    "#     if timerCount % 10000 == 0:\n",
    "#         avgTimeInterleavingInMs = 1000*totalTimeInterleaving / timerCount\n",
    "#         avgTimeClickingInMs = 1000*totalTimeClicking / timerCount\n",
    "#         avgTimeEvaluatingInMs = 1000*totalTimeEvaluating / timerCount\n",
    "#         timerCount = 0\n",
    "#         totalTimeInterleaving = 0.0\n",
    "#         totalTimeClicking = 0.0\n",
    "#         totalTimeEvaluating = 0.0\n",
    "#         print(\"interleaving: {:.3f}ms \\tclicking: {:.3f}ms \\teval: {:.3f}ms\".format(avgTimeInterleavingInMs, avgTimeClickingInMs, avgTimeEvaluatingInMs))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def N_simulation(pair, click_model, N):\n",
    "    A = 0 #first search engine score\n",
    "    B = 0 #second search engine score\n",
    "    \n",
    "    for i in range(N):\n",
    "        result = simulation_for_pair(pair, click_model)\n",
    "        if result > 0:\n",
    "            A+=1\n",
    "        if result < 0:\n",
    "            B+=1\n",
    "    return A, B\n",
    "\n",
    "A_score, B_score = N_simulation(pair, \"RCM\", 100)\n",
    "print(A_score, B_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Results and Analysis (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the code in this function is just a copy of the step1-3 code\n",
    "def getDeltaMeasures (P, E, k):\n",
    "    totalCounter = Counter(P) + Counter(E)\n",
    "    relevantDocumentCount = totalCounter['R'] + totalCounter['HR']\n",
    "    \n",
    "    # implement 1 of (binary):\n",
    "    #   precision at rank k\n",
    "    #   recall at rank k\n",
    "    #   average precision   <--\n",
    "    averagePrecisionP = getAveragePrecision(P, relevantDocumentCount)\n",
    "    averagePrecisionE = getAveragePrecision(E, relevantDocumentCount)\n",
    "    \n",
    "    # save for calculating MAP later\n",
    "    averagePrecisionsForMapP.append(averagePrecisionP)\n",
    "    averagePrecisionsForMapE.append(averagePrecisionE)\n",
    "    \n",
    "    # Normalized Discounted Cumulative Gain\n",
    "    # First we have to determine the perfect ranking. Assuming the P and E results are always\n",
    "    # different, and that both algorithms run on the same corpus of documents, the perfect ranking \n",
    "    # would include the results from both rankings.\n",
    "    k = 3\n",
    "    mergedRanking = P + E\n",
    "    perfectRanking = sorted(mergedRanking, key=lambda relevance: relevanceScores[relevance], reverse=True)\n",
    "    perfectDcgScore = getDiscountedCumulativeGain(perfectRanking[:k])\n",
    "    dcgAtKP = getDiscountedCumulativeGain(P[:k])\n",
    "    dcgAtKE = getDiscountedCumulativeGain(E[:k])\n",
    "    nDcgAtKP = dcgAtKP / perfectDcgScore\n",
    "    nDcgAtKE = dcgAtKE / perfectDcgScore\n",
    "    \n",
    "    \n",
    "    # Expected Reciprocal Rank\n",
    "    errP = getExpectedReciprocalRank(P)\n",
    "    errE = getExpectedReciprocalRank(E)\n",
    "    \n",
    "    # calculate delta measures\n",
    "    deltaAp = averagePrecisionE - averagePrecisionP\n",
    "    deltaNDcgAtK = nDcgAtKE - nDcgAtKP\n",
    "    deltaErr = errE - errP\n",
    "    \n",
    "    return deltaAp, deltaNDcgAtK, deltaErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing test set #0 out of 1000 at 0.000 sets per second\n",
      "Analyzing test set #100 out of 1000 at 7.103 sets per second\n",
      "Analyzing test set #200 out of 1000 at 7.176 sets per second\n",
      "Analyzing test set #300 out of 1000 at 7.103 sets per second\n",
      "Analyzing test set #400 out of 1000 at 7.075 sets per second\n",
      "Analyzing test set #500 out of 1000 at 7.097 sets per second\n",
      "Analyzing test set #600 out of 1000 at 7.114 sets per second\n",
      "Analyzing test set #700 out of 1000 at 7.123 sets per second\n",
      "Analyzing test set #800 out of 1000 at 7.115 sets per second\n",
      "Analyzing test set #900 out of 1000 at 7.122 sets per second\n",
      "\tAP\tnDCG\tERR\tRCM\tSDCM\t\n",
      "AP:\t100%\t67%\t65%\t44%\t61%\t\n",
      "nDCG:\t67%\t100%\t91%\t46%\t80%\t\n",
      "ERR:\t65%\t91%\t100%\t48%\t83%\t\n",
      "RCM:\t44%\t46%\t48%\t100%\t47%\t\n",
      "SDCM:\t61%\t80%\t83%\t47%\t100%\t\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "startTime = timer()\n",
    "\n",
    "# perform an experiment to find correlation between each pair of measures\n",
    "k = 5\n",
    "numberOfRandomTestSets = 1000\n",
    "testSetSize = 1\n",
    "numberOfInterleavingExperimentsPerPair = 100\n",
    "measures = ['AP', 'nDCG', 'ERR', 'RCM', 'SDCM']\n",
    "measurePairAgreementCounts = {}\n",
    "for measureA in measures:\n",
    "    measurePairAgreementCounts[measureA] = {}\n",
    "    for measureB in measures:\n",
    "        measurePairAgreementCounts[measureA][measureB] = 0\n",
    "        \n",
    "for testSetNumber in range(numberOfRandomTestSets):\n",
    "    if int(100000.0 * (testSetNumber / numberOfRandomTestSets)) % 10000 == 0:\n",
    "        # every 10 percent\n",
    "        print(\"Analyzing test set #{} out of {} at {:.3f} sets per second\".format(testSetNumber, numberOfRandomTestSets, testSetNumber/(timer()-startTime)))\n",
    "    \n",
    "    # because the whole testset is symmetric, we randomly pick a subset which is \n",
    "    # almost guaranteed to be imbalanced (one algorithm is better than the other)\n",
    "    shuffle(pairsOfRankingsOf5)\n",
    "    testset = pairsOfRankingsOf5[:testSetSize]\n",
    "    \n",
    "    # count the number of wins of each algorithm for each pair in this test set\n",
    "    winCounts = { \n",
    "        'AP':[0,0,0],\n",
    "        'nDCG':[0,0,0],\n",
    "        'ERR':[0,0,0],\n",
    "        'RCM':[0,0,0],\n",
    "        'SDCM':[0,0,0]\n",
    "    }\n",
    "    for pair in testset:\n",
    "        measureResults = {}\n",
    "        # collect 5 measures for this pair\n",
    "        P = pair[0]\n",
    "        E = pair[1]\n",
    "        measureResults['AP'], measureResults['nDCG'], measureResults['ERR'] = getDeltaMeasures(P, E, k)\n",
    "        scoreRcmA, scoreRcmB = N_simulation(pair, \"RCM\", numberOfInterleavingExperimentsPerPair)\n",
    "        measureResults['RCM'] = scoreRcmB - scoreRcmA\n",
    "        scoreSdcmA, scoreSdcmB = N_simulation(pair, \"SDCM\", numberOfInterleavingExperimentsPerPair)\n",
    "        measureResults['SDCM'] = scoreSdcmB - scoreSdcmA\n",
    "        \n",
    "        # determine winner for each measure\n",
    "        epsilon = 1e-6 # avoid floating point imprecisions\n",
    "        for measure, result in measureResults.items():\n",
    "            if result > epsilon:\n",
    "                winCounts[measure][2] += 1 # E wins\n",
    "            elif result < -epsilon:\n",
    "                winCounts[measure][0] += 1 # P wins\n",
    "            else:\n",
    "                winCounts[measure][1] += 1 # draw\n",
    "        \n",
    "    # after counting all results for the entire test set, determine overall winner for each measure\n",
    "    winners = {}\n",
    "    for measure, counts in winCounts.items():\n",
    "        if winCounts[measure][0] > winCounts[measure][2]:\n",
    "            winners[measure] = 'P'\n",
    "        elif winCounts[measure][0] < winCounts[measure][2]:\n",
    "            winners[measure] = 'E'\n",
    "        else:\n",
    "            winners[measure] = 'draw'\n",
    "    \n",
    "    # after determining the winners of the test set, see which measures agree with each other\n",
    "    for measureA, winnerA in winners.items():\n",
    "        for measureB, winnerB in winners.items():\n",
    "            if winnerA == winnerB:\n",
    "                measurePairAgreementCounts[measureA][measureB] += 1\n",
    "\n",
    "# after running so many randomized test sets, see how much each measure lines up with the others\n",
    "rowString = \"\\t\"\n",
    "for measure in measures:\n",
    "    rowString += \"{}\\t\".format(measure)\n",
    "print(rowString)\n",
    "\n",
    "for measureA, otherMeasures in measurePairAgreementCounts.items():\n",
    "    rowString = \"{}:\\t\".format(measureA)\n",
    "    for measureB, agreementCount in otherMeasures.items():\n",
    "        rowString += \"{:.0%}\\t\".format(agreementCount/numberOfRandomTestSets)\n",
    "    print(rowString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
