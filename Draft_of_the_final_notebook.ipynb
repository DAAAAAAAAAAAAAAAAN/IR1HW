{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Hypothesis Testing - the problem of multiple comparisons (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type I error occurs in situations where the null hypothesis is rejected while $H_0$ is true and must not be rejected. Thus, the probability of making an incorrect decision on the statisitical significance for a given experiment is $P ($type I error$) = alpha$. Conversely, the probability of the correct rejection of the null hypothesis is $P ($correct $H_0$ rejection$) = 1 - alpha$. <br><br>\n",
    "_Main question w.r.t. (a): are we supposed to estimate the prob of (one?) m-th significant result or (one?) m-th FALSE significant result as follows from the plot in slide 35, lecture 2b?_ <br>\n",
    "(a) <br>Interpretation 1:<br>\n",
    "$P ($not making a type I error in $(m - 1)$ experiments but making it in the m-th experiment alone$) = (1 - alpha)^{m-1}*alpha$<br>\n",
    "Interpretation 2:<br>\n",
    "$P ($not making a type I error in $m$ experiments$) = (1 - alpha)^m$<br>\n",
    "\n",
    "(b) For a series of $m$ experiments, we can estimate the probability of having performed at least one significant experiment as the inverse probability of making Type I error in all $m$ tests. Thus, <br>\n",
    "Interpretation 1:<br>\n",
    "$P ($type I error in one experiment$) = alpha$<br>\n",
    "$P ($type I error in all $m$ experiments$) = alpha^m$<br>\n",
    "$P ($at least one significant experiment$) = 1 - alpha^m$<br>\n",
    "Interpretation 2 (one FALSE significant result):<br>\n",
    "$P ($type I error in one experiment$) = alpha$<br>\n",
    "$P ($not making a type I error in one experiment$) = 1 - alpha$<br>\n",
    "$P ($not making a type I error in all $m$ experiments$) = {(1 - alpha)}^m$<br>\n",
    "$P ($at least one FALSE significant experiment in $m$ experiments$) = 1 - {(1 - alpha)}^m$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Bias and unfairness in Interleaving experiments (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team-draft interleaving method makes an incorrect judgement when, for instance, there exists only one relevant (clicked) document and it is ranked differently by two different algorithms. <br>\n",
    "Let us consider the following example. Given two ranking algorithms A and B, we assume that these algorithms operate on a collection of three documents ($d_1$, $d_2$, $d_3$) where only one of them (say, $d_3$, is relevant). Hence, algorithm A ranks the documents in the following order: $d_1: N$, $d_2:N$, $d_3:R$ while algorithm B ranks them as follows: $d_2: N$, $d_3:R$, $d_1:N$. Intuitively, algorithm B must win the competition as it assigns a higher rank to the only relevant document. However, this does not take place in reality if the team-draft interleaving method is applied. <br>\n",
    "As the order of picking the documents is randomized, there exist four possible assignments (\"ABA\", \"ABB\", \"BAA\", \"BAB\"), which represent the order in which each algorithm (encoded as a literal in a given sequence) picks a preferred document. The resulting rankings are as follows: <br><br>\n",
    "**ABA**: \n",
    "> {$d_1: N$} (from A), {$d_2: N$} (from B), {$d_3: R$} (from A)\n",
    "\n",
    "**ABB**: \n",
    "> {$d_1: N$} (from A), {$d_2: N$} (from B), {$d_3: R$} (from B)\n",
    "\n",
    "**BAA**: \n",
    "> {$d_2: N$} (from B), {$d_1: N$} (from A), {$d_3: R$} (from A)\n",
    "\n",
    "**BAB**: \n",
    "> {$d_2: N$} (from B), {$d_1: N$} (from A), {$d_3: R$} (from B)\n",
    "\n",
    "We can observe that the only relevant document $d_3$ is ranked to appear third in all the assignments. As a result, the team-draft method considers this situation a tie while it is clear from the initial conditions that algorithm B should be preferred over algorithm A. In conclusion, the case described above represents a situation where the team-draft interleaving method incorrectly evaluates given ranking algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Rankings of Relevance for _E_ and _P_ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given three relevance labels (N, R, and HR), we assume the following relevance scores. Thus, non-relevant documents have a zero score, relevant documents have a score of 1, highly relevant documents are scored 2. Assuming ranking pairs to be of length 5, we then generate $3^5 = 243$ possible rankings for production algorithm P and experimental algorithm E.<br>We can represent the given rankings as an $(n*n)$-matrix, where $n$ is the number of possible rankings for P and E. Given this representation, we can conclude that there must exist $243 * 243 = 59049$ possible ranking pairs of P and E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59049, 'pairs of rankings')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "import numpy\n",
    "import random\n",
    "import math\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "k = 3\n",
    "relevances = { 'N', 'R', 'HR' }\n",
    "relevanceScores = { 'N':0, 'R':1, 'HR':5 }\n",
    "rankingsOf5 = list(itertools.product(relevances, repeat=5))\n",
    "pairsOfRankingsOf5 = list(itertools.product(rankingsOf5, rankingsOf5))\n",
    "shuffle(pairsOfRankingsOf5)\n",
    "print(len(pairsOfRankingsOf5), \"pairs of rankings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Evaluation Measures (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the evaluation metrics, we have selected to implement average precision, normalised discounted cumulative gain at rank _k_ (nDCG@k), and expected reciprocal rank (ERR). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getContingencies (items, k, relevantDocumentCount):\n",
    "    retrievedCounter = Counter(items[:k])\n",
    "    TP = retrievedCounter['R'] + retrievedCounter['HR']\n",
    "    FP = retrievedCounter['N']\n",
    "    \n",
    "    notRetrievedCounter = Counter(items[k:])\n",
    "    TN = notRetrievedCounter['N']\n",
    "    FN = relevantDocumentCount - TP\n",
    "    \n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def getPrecisionAtK (ranking, k):\n",
    "    TP, FP, TN, FN = getContingencies(ranking, k, relevantDocumentCount)\n",
    "    precisionAtK = TP / (TP + FP)\n",
    "#     recallAtK = TP / (TP + FN)\n",
    "#     F1AtK = 2*precisionAtK*recallAtK\n",
    "#     if F1AtK > 0.0: \n",
    "#         F1AtK /= precisionAtK + recallAtK\n",
    "#     accuracyAtK = (TP + TN)/(TP + FP + FN + TN)\n",
    "    return precisionAtK\n",
    "\n",
    "def getAveragePrecision (ranking, relevantDocumentCount):\n",
    "    precisionsForAp = []\n",
    "    for k in range(1, len(ranking)+1):\n",
    "        precisionAtK = getPrecisionAtK(ranking, k)\n",
    "\n",
    "        if ranking[k-1] == 'R' or ranking[k-1] == 'HR':\n",
    "            # save for calculating AP later\n",
    "            precisionsForAp.append(precisionAtK)\n",
    "    \n",
    "    averagePrecision = sum(precisionsForAp)/relevantDocumentCount\n",
    "    return averagePrecision\n",
    "\n",
    "def getDiscountedCumulativeGain (ranking):\n",
    "    dcg = 0.0\n",
    "    for r in range(1, len(ranking)+1):\n",
    "        relevanceAtR = relevanceScores[ranking[r-1]]\n",
    "        gain = (2 ** relevanceAtR) - 1\n",
    "        discount = math.log2(1 + r)\n",
    "        dcg += gain/discount\n",
    "    return dcg\n",
    "\n",
    "def getProbabilityOfRelevance (relevance):\n",
    "    # from paper, similar to DCG score\n",
    "    gain = (2 ** relevanceScores[relevance]) - 1\n",
    "    discount = 2 ** max(relevanceScores.values())\n",
    "    probability = gain / discount\n",
    "#     print('probability of', relevance, 'is', probability)\n",
    "    return probability\n",
    "\n",
    "def getExpectedReciprocalRank (ranking):\n",
    "    err = 0.0\n",
    "    for r in range(1, len(ranking)+1):\n",
    "        probabilityOfReachingRankR = 1.0\n",
    "        for j in range(r-1):\n",
    "            probabilityOfReachingRankR *= 1 - getProbabilityOfRelevance(ranking[j])\n",
    "        probabilityOfStoppingAtRankR = getProbabilityOfRelevance(ranking[r-1])\n",
    "        probabilityOfSatisfaction = probabilityOfReachingRankR * probabilityOfStoppingAtRankR\n",
    "        expectedProbabilityOfSatisfaction = probabilityOfSatisfaction / r\n",
    "        err += expectedProbabilityOfSatisfaction\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate $\\delta$-measure (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the next step, we count the number of pairs where the experimental algorithm outperforms the production algorithm and calculate the difference in performance with respect to all the three pre-selected evaluation metrics for such pairs (we ignore one ranking pair where all the P and E's documents are irrelevant). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nP: ', ('N', 'R', 'N', 'N', 'N'), '\\nE: ', ('R', 'N', 'N', 'R', 'HR'))\n",
      "('perfect ranking:\\t', ['HR', 'R', 'R', 'R', 'N', 'N', 'N', 'N', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 32.13092975357146)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.020 \tE:0.031\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('HR', 'R', 'R', 'R', 'HR'), '\\nE: ', ('R', 'N', 'R', 'N', 'R'))\n",
      "('perfect ranking:\\t', ['HR', 'HR', 'R', 'R', 'R', 'R', 'R', 'R', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 51.05882236071518)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.629 \tE:0.029\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('HR', 'N', 'N', 'HR', 'N'), '\\nE: ', ('N', 'HR', 'HR', 'R', 'R'))\n",
      "('perfect ranking:\\t', ['HR', 'HR', 'HR', 'HR', 'R', 'R', 'N', 'N', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 66.05882236071518)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.469 \tE:0.531\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('HR', 'R', 'R', 'HR', 'HR'), '\\nE: ', ('R', 'R', 'R', 'N', 'HR'))\n",
      "('perfect ranking:\\t', ['HR', 'HR', 'HR', 'HR', 'R', 'R', 'R', 'R', 'R', 'N'])\n",
      "('perfect DCG score:\\t', 66.05882236071518)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.486 \tE:0.032\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('N', 'R', 'N', 'R', 'HR'), '\\nE: ', ('HR', 'N', 'N', 'N', 'HR'))\n",
      "('perfect ranking:\\t', ['HR', 'HR', 'HR', 'R', 'R', 'N', 'N', 'N', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 66.05882236071518)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.010 \tE:0.469\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('N', 'N', 'N', 'N', 'HR'), '\\nE: ', ('N', 'R', 'R', 'R', 'R'))\n",
      "('perfect ranking:\\t', ['HR', 'R', 'R', 'R', 'R', 'N', 'N', 'N', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 32.13092975357146)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.000 \tE:0.035\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('N', 'R', 'N', 'R', 'HR'), '\\nE: ', ('R', 'R', 'N', 'HR', 'R'))\n",
      "('perfect ranking:\\t', ['HR', 'HR', 'R', 'R', 'R', 'R', 'R', 'N', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 51.05882236071518)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.012 \tE:0.032\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('N', 'N', 'HR', 'N', 'R'), '\\nE: ', ('N', 'R', 'N', 'HR', 'HR'))\n",
      "('perfect ranking:\\t', ['HR', 'HR', 'HR', 'R', 'R', 'N', 'N', 'N', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 66.05882236071518)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.235 \tE:0.010\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('HR', 'R', 'N', 'R', 'R'), '\\nE: ', ('N', 'R', 'N', 'R', 'R'))\n",
      "('perfect ranking:\\t', ['HR', 'R', 'R', 'R', 'R', 'R', 'R', 'N', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 32.13092975357146)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.984 \tE:0.020\n",
      "ERR: \tP:0.000 \tE:0.000\n",
      "('\\nP: ', ('HR', 'R', 'R', 'R', 'R'), '\\nE: ', ('N', 'N', 'N', 'N', 'HR'))\n",
      "('perfect ranking:\\t', ['HR', 'HR', 'R', 'R', 'R', 'R', 'N', 'N', 'N', 'N'])\n",
      "('perfect DCG score:\\t', 51.05882236071518)\n",
      "AP: \tP:0.000 \tE:0.000\n",
      "nDCG: \tP:0.629 \tE:0.000\n",
      "ERR: \tP:0.000 \tE:0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# print results\\n\\n# print how many times E outperformed P\\nprint('\\n\\nOut of {} rankings at k = {}, E outperformed P:'.format(len(pairsOfRankingsOf5)-1, k))\\nprint('AP: \\t{:.3%}'.format(pairCountForWhichEHasBetterAp/len(pairsOfRankingsOf5)))\\nprint('nDCG: \\t{:.3%}'.format(pairCountForWhichEHasBetterNDcg/len(pairsOfRankingsOf5)))\\nprint('ERR: \\t{:.3%}'.format(pairCountForWhichEHasBetterErr/len(pairsOfRankingsOf5)))\\n        \\n# we accidentally implemented MAP instead of just AP, but we'll leave it in\\nmeanAveragePrecisionP = sum(averagePrecisionsForMapP)/len(averagePrecisionsForMapP)\\nmeanAveragePrecisionE = sum(averagePrecisionsForMapE)/len(averagePrecisionsForMapE)\\nprint('MAP \\tP:{:.3f} \\tE:{:.3f}'.format(meanAveragePrecisionP, meanAveragePrecisionE))\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averagePrecisionsForMapP = []\n",
    "averagePrecisionsForMapE = []\n",
    "pairCountForWhichEHasBetterAp = 0\n",
    "pairCountForWhichEHasBetterNDcg = 0\n",
    "pairCountForWhichEHasBetterErr = 0\n",
    "for i, rankingPair in enumerate(pairsOfRankingsOf5):\n",
    "    P = rankingPair[0]\n",
    "    E = rankingPair[1]\n",
    "\n",
    "    # implement 1 of (binary):\n",
    "    #   precision at rank k\n",
    "    #   recall at rank k\n",
    "    #   average precision   <--\n",
    "    totalCounter = Counter(P) + Counter(E)\n",
    "    relevantDocumentCount = totalCounter['R'] + totalCounter['HR']\n",
    "    \n",
    "    if relevantDocumentCount == 0:\n",
    "        # result is irrelevant\n",
    "        continue\n",
    "    \n",
    "    averagePrecisionP = getAveragePrecision(P, relevantDocumentCount)\n",
    "    averagePrecisionE = getAveragePrecision(E, relevantDocumentCount)\n",
    "    \n",
    "    # save for calculating MAP later\n",
    "    averagePrecisionsForMapP.append(averagePrecisionP)\n",
    "    averagePrecisionsForMapE.append(averagePrecisionE)\n",
    "\n",
    "    # implement 2 of (multi-graded):\n",
    "    #   nDCG at rank k\n",
    "    #   ERR\n",
    "    \n",
    "    # Normalized Discounted Cumulative Gain\n",
    "    # First we have to determine the perfect ranking. Assuming the P and E results are always\n",
    "    # different, and that both algorithms run on the same corpus of documents, the perfect ranking \n",
    "    # would include the results from both rankings.\n",
    "    mergedRanking = P + E\n",
    "    perfectRanking = sorted(mergedRanking, key=lambda relevance: relevanceScores[relevance], reverse=True)\n",
    "    perfectDcgScore = getDiscountedCumulativeGain(perfectRanking[:k])\n",
    "    dcgAtKP = getDiscountedCumulativeGain(P[:k])\n",
    "    dcgAtKE = getDiscountedCumulativeGain(E[:k])\n",
    "    nDcgAtKP = dcgAtKP / perfectDcgScore\n",
    "    nDcgAtKE = dcgAtKE / perfectDcgScore\n",
    "    \n",
    "    \n",
    "    # Expected Reciprocal Rank\n",
    "    errP = getExpectedReciprocalRank(P[:k])\n",
    "    errE = getExpectedReciprocalRank(E[:k])\n",
    "    \n",
    "    # calculate delta measures\n",
    "    deltaAp = averagePrecisionE - averagePrecisionP\n",
    "    deltaNDcg = nDcgAtKE - nDcgAtKP\n",
    "    deltaErr = errE - errP\n",
    "    \n",
    "    # count pairs for which E outperforms P\n",
    "    epsilon = 1e-6 # avoid floating point imprecisions\n",
    "    if deltaAp > epsilon:\n",
    "        pairCountForWhichEHasBetterAp += 1\n",
    "    if deltaNDcg > epsilon:\n",
    "        pairCountForWhichEHasBetterNDcg += 1\n",
    "    if deltaErr > epsilon:\n",
    "        pairCountForWhichEHasBetterErr += 1\n",
    "    \n",
    "    # only show a few\n",
    "    if i < 10:\n",
    "        # show the pair\n",
    "        print ('\\nP: ', P, '\\nE: ', E)\n",
    "        print('perfect ranking:\\t', perfectRanking)\n",
    "        print('perfect DCG score:\\t', perfectDcgScore)\n",
    "        print('AP: \\tP:{:.3f} \\tE:{:.3f}'.format(averagePrecisionP, averagePrecisionE))\n",
    "        print('nDCG: \\tP:{:.3f} \\tE:{:.3f}'.format(nDcgAtKP, nDcgAtKE))\n",
    "        print('ERR: \\tP:{:.3f} \\tE:{:.3f}'.format(errP, errE))\n",
    "        \n",
    "\"\"\"\n",
    "# print results\n",
    "\n",
    "# print how many times E outperformed P\n",
    "print('\\n\\nOut of {} rankings at k = {}, E outperformed P:'.format(len(pairsOfRankingsOf5)-1, k))\n",
    "print('AP: \\t{:.3%}'.format(pairCountForWhichEHasBetterAp/len(pairsOfRankingsOf5)))\n",
    "print('nDCG: \\t{:.3%}'.format(pairCountForWhichEHasBetterNDcg/len(pairsOfRankingsOf5)))\n",
    "print('ERR: \\t{:.3%}'.format(pairCountForWhichEHasBetterErr/len(pairsOfRankingsOf5)))\n",
    "        \n",
    "# we accidentally implemented MAP instead of just AP, but we'll leave it in\n",
    "meanAveragePrecisionP = sum(averagePrecisionsForMapP)/len(averagePrecisionsForMapP)\n",
    "meanAveragePrecisionE = sum(averagePrecisionsForMapE)/len(averagePrecisionsForMapE)\n",
    "print('MAP \\tP:{:.3f} \\tE:{:.3f}'.format(meanAveragePrecisionP, meanAveragePrecisionE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated the $\\delta$-measures, we proceed by implementing the balanced interleaving method and the team-draft interleaving method for online evaluation of the ranking algorithms. As P and E are assumed to always return different documents, we additionally index each document considered within a ranking pair to distinguish them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ranking pair: ', [['HR', 'HR', 'R', 'R', 'N'], ['HR', 'R', 'R', 'N', 'N']])\n",
      "('Updated pair: ', (['HR0', 'HR1', 'R2', 'R3', 'N4'], ['HR5', 'R6', 'R7', 'N8', 'N9']))\n",
      "('interleaved: ', [['HR0', 0], ['HR5', 1], ['HR1', 0], ['R6', 1], ['R7', 1], ['R2', 0], ['N8', 1], ['R3', 0], ['N9', 1], ['N4', 0]])\n",
      "('Random Clicks: ', [1, 4, 7])\n",
      "('Evaluation A-B (number of clicks): ', -1)\n"
     ]
    }
   ],
   "source": [
    "###### STEP 4 ######\n",
    "\n",
    "#Team Draft Interleaving\n",
    "\n",
    "\n",
    "#to be able to distinguish the document's assignment, rename the relevances by adding index number\n",
    "#eg. Ranking pair:  [['HR', 'HR', 'R', 'R', 'N'], ['HR', 'R', 'R', 'N', 'N']]\n",
    "# Updated pair : (['HR0', 'HR1', 'R2', 'R3', 'N4'], ['HR5', 'R6', 'R7', 'N8', 'N9'])\n",
    "\n",
    "def preprocess(pair):\n",
    "    i = 0\n",
    "    updated_pair = ([], [])\n",
    "    for relevance in pair[0]:\n",
    "        updated_pair[0].append(relevance+ str(i))\n",
    "        i+=1\n",
    "    \n",
    "    for relevance in pair[1]:\n",
    "        updated_pair[1].append(relevance+ str(i))\n",
    "        i+=1        \n",
    "    \n",
    "    return updated_pair\n",
    "\n",
    "\n",
    "#a) create the interleaving list\n",
    "def team_draft_interleave(pair):\n",
    "    pair = preprocess(pair)\n",
    "    \n",
    "    A = pair[0][:]\n",
    "    B = pair[1][:]\n",
    "    doc_list = []  #the interleaving list\n",
    "    assignments = [] #holds the pair that the documents taken from, \"A or B\", \"0 or 1\" respectively\n",
    "    while A or B:\n",
    "        if A and B:\n",
    "            first = random.randint(0,1)\n",
    "        elif A:\n",
    "            first = 0\n",
    "        else:\n",
    "            first = 1\n",
    "        second = int(math.fabs(first-1))\n",
    "\n",
    "        #pick from the first list(A) then the second list (B)\n",
    "        if first == 0: #A\n",
    "            doc = A[0]\n",
    "            doc_list.append(doc)\n",
    "            assignments.append(first)\n",
    "            #delete the doc from the first and the second list\n",
    "            del A[0] \n",
    "            if doc in B:\n",
    "                B.remove(doc)\n",
    "            \n",
    "            #pick from the second list\n",
    "            doc = B[0]\n",
    "            doc_list.append(doc)\n",
    "            assignments.append(second) \n",
    "            #delete the doc from the second list (top element) and the first list\n",
    "            del B[0]\n",
    "            \n",
    "            if doc in A:\n",
    "                A.remove(doc)\n",
    "            \n",
    "       #pick from the first list(B) then the second list (A)\n",
    "        else: #B\n",
    "            doc = B[0]\n",
    "            doc_list.append(doc)\n",
    "            assignments.append(first)\n",
    "            del B[0]\n",
    "            if doc in A:\n",
    "                A.remove(doc)\n",
    "\n",
    "            #pick from the second list\n",
    "            doc = A[0]\n",
    "            doc_list.append(doc)\n",
    "            assignments.append(second)\n",
    "            #delete the doc from the first and the second list\n",
    "            del A[0] \n",
    "            if doc in B:\n",
    "                B.remove(doc)\n",
    "    \n",
    "       \n",
    "    #append the interleaved list with the assignment        \n",
    "    output = []\n",
    "    for i in range(len(doc_list)):\n",
    "        output.append([doc_list[i],assignments[i]])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "#generate a list of 'number' amount of random clicks\n",
    "def random_clicks(length, number):\n",
    "    clicks = random.sample(range(length), number)\n",
    "    return clicks\n",
    "\n",
    "    \n",
    "#b) evaluate the interleaved list with random clicks\n",
    "\n",
    "\n",
    "def team_draft_evaluation(interleaved, clicks):\n",
    "    doc_list, assignments = zip(*interleaved) \n",
    "#     print(doc_list)\n",
    "#     print(assignments)\n",
    "    click_nums = [0, 0]\n",
    "    for i in clicks:\n",
    "        click_nums[assignments[i]]+=1\n",
    "\n",
    "    return click_nums[0]-click_nums[1]\n",
    "\n",
    "\n",
    "pair = [[\"HR\", \"HR\", \"R\", \"R\", \"N\"], [\"HR\", \"R\", \"R\", \"N\", \"N\"]]\n",
    "print (\"Ranking pair: \", pair)\n",
    "updated_pair = preprocess(pair)\n",
    "print(\"Updated pair: \", updated_pair)\n",
    "interleaved = team_draft_interleave(pair)\n",
    "print(\"interleaved: \", interleaved)\n",
    "\n",
    "clicks = random_clicks(len(interleaved), 3)\n",
    "print(\"Random Clicks: \", clicks)\n",
    "\n",
    "result = team_draft_evaluation(interleaved, clicks)\n",
    "print(\"Evaluation A-B (number of clicks): \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement User Clicks Simulation (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiment, we consider two click models: Random Click Model (RCM) and Simple Dependent Click Model (SDCM). Having browsed the Yandex Click Log File, we observed that there existed some relevant documents had been clicked multiple times. As the selected click models do not take into account multiple clicks for one document, we only consider the first click on such relevant documents and skip the rest.<br>Attractiveness is estimated as the probability of relevance (__refer to the paper here__). Satisfaction is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.126312951327\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "### STEP 5 ####\n",
    "\n",
    "# Yandex Click Log File\n",
    "def get_sessions():\n",
    "    f = open('YandexRelPredChallenge.txt', 'r')\n",
    "    content = []\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        content.append(line)\n",
    "    f.close()\n",
    "\n",
    "    #print(content[:10])\n",
    "\n",
    "    #each session is a query that has the list of documents retrieved and list of documents that are clicked\n",
    "    sessions = []  \n",
    "    for line in content:\n",
    "        if(line[2]) == 'Q': \n",
    "            session = []\n",
    "            session.append(line[5:])\n",
    "            session.append([])\n",
    "            sessions.append(session)\n",
    "        if(line[2]) == 'C':\n",
    "            c = line[-1] \n",
    "            for i in range(-1, -len(sessions), -1): #attribute the click to the last query that had this document as a result\n",
    "                session = sessions[i]\n",
    "                if c in session[0]:\n",
    "                    sessions[i][1].append(c)\n",
    "                    break;\n",
    "    return sessions\n",
    "\n",
    "sessions = get_sessions()\n",
    "\n",
    "#Random Click Model\n",
    "\n",
    "\n",
    "#parameter from the user log\n",
    "def RCM_parameter(sessions):\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    for s in sessions:\n",
    "        clicks = set()\n",
    "        for c in s[1]:\n",
    "            clicks.add(c)\n",
    "        sum1 += len(clicks)\n",
    "        sum2 += len(s[0])\n",
    "        \n",
    "    p = 1.0*sum1/sum2 \n",
    "        \n",
    "    return p\n",
    "\n",
    "\n",
    "def RCM_clicks(length, p):\n",
    "    clicks = []\n",
    "    for i in range(length):\n",
    "        prob = random.uniform(0, 1)\n",
    "        if prob < p:\n",
    "            clicks.append(i)\n",
    "    return clicks\n",
    "\n",
    "p = RCM_parameter(sessions)\n",
    "print(p)\n",
    "\n",
    "clicks = RCM_clicks(10, p)\n",
    "print(clicks)\n",
    "\n",
    "interleaved = [['HR0', 0], ['HR5', 1], ['R6', 1], ['HR1', 0], ['R7', 1], ['R2', 0], ['N8', 1], ['R3', 0], ['N9', 1], ['N4', 0]]\n",
    "\n",
    "def getProbabilityOfRelevance (relevance):\n",
    "    # from paper, similar to DCG score\n",
    "    gain = (2 ** relevanceScores[relevance]) - 1\n",
    "    discount = 2 ** max(relevanceScores.values())\n",
    "    probability = gain / discount\n",
    "    return probability\n",
    "\n",
    "def attractiveness(interleaved):\n",
    "    attr = []\n",
    "    for doc in interleaved:\n",
    "        relevance = doc[0][0]\n",
    "        if relevance == 'H':\n",
    "            attr.append(getProbabilityOfRelevance (\"HR\"))\n",
    "        if relevance == 'R':\n",
    "            attr.append(getProbabilityOfRelevance (\"R\"))\n",
    "        if relevance == 'N':\n",
    "            attr.append(getProbabilityOfRelevance (\"N\"))\n",
    "    return attr\n",
    "    \n",
    "\n",
    "\n",
    "def lambda_r(sessions, r):\n",
    "    S = list(filter(lambda s: s[0][r-1] in s[1] , sessions))\n",
    "    total = 0\n",
    "    for s in S:\n",
    "        click_index = [s[0].index(page) for page in s[1]]\n",
    "        last_click = click_index[-1] + 1 #satisfied\n",
    "        if last_click != r:\n",
    "            total += 1\n",
    "\n",
    "    return total/float(len(S))  \n",
    "    \n",
    "\n",
    "def get_lambdas(sessions):\n",
    "    lambdas = []\n",
    "    for i in range(10):\n",
    "        lambdas.append(lambda_r(sessions, i+1))\n",
    "    return lambdas\n",
    "    \n",
    "\n",
    "def SDCM_clicks(interleaved, lambdas):\n",
    "    clicks = []\n",
    "    attr = attractiveness(interleaved)\n",
    "    n = len(attr)\n",
    "    for i in range(1, n):\n",
    "        prob = random.uniform(0,1)\n",
    "        \n",
    "        if prob < attr[i]: #clicked\n",
    "            clicks.append(i)\n",
    "            prob = random.uniform(0,1)\n",
    "            if prob > lambdas[i]: #stop\n",
    "                break\n",
    "                \n",
    "    return clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Simulate Interleaving Experiment (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Justification for the code in the section below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambdas:[0.31942931258106355, 0.5378201310303752, 0.5706431189603466, 0.58397365532382, 0.5779661016949152, 0.5579487179487179, 0.5545207605743112, 0.5058619192357794, 0.468378506894912, 0.22727272727272727]\n",
      "(29, 30)\n"
     ]
    }
   ],
   "source": [
    "###### STEP 6 #####\n",
    "\n",
    "# For simulation, we need to run the following piece of code for each pair to get the score\n",
    "# result : which search engine has higher clicks, A - B, eg. if it is higher than 0, the first one wins\n",
    "# each run has a different result, so \n",
    "\n",
    "pair = [[\"HR\", \"HR\", \"R\", \"R\", \"N\"], [\"HR\", \"R\", \"R\", \"N\", \"N\"]]\n",
    "sessions = get_sessions()   #gets the sessions as a list from yandex log file\n",
    "p = RCM_parameter(sessions) #calculates the RCM parameter, MLE\n",
    "lambdas = get_lambdas(sessions)  #calculates all the lambda values for each rank (1-10)\n",
    "print(\"lambdas:{}\".format(lambdas))\n",
    "\n",
    "def simulation_for_pair(pair, click_model):\n",
    "    interleaved = team_draft_interleave(pair)  #gets the interleaved list\n",
    "\n",
    "    clicks_RCM = RCM_clicks(10, p)  #generates clicks based on RCM model\n",
    "\n",
    "    clicks_SDCM = SDCM_clicks(interleaved, lambdas) #generates clicks based on RCM model\n",
    "\n",
    "    if click_model == \"RCM\":\n",
    "        result = team_draft_evaluation(interleaved, clicks_RCM)\n",
    "    if click_model == \"SDCM\":\n",
    "        result = team_draft_evaluation(interleaved, clicks_SDCM)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def N_simulation(pair, click_model, N):\n",
    "    A = 0 #first search engine score\n",
    "    B = 0 #second search engine score\n",
    "    \n",
    "    for i in range(N):\n",
    "        result = simulation_for_pair(pair, click_model)\n",
    "        if result > 0:\n",
    "            A+=1\n",
    "        if result < 0:\n",
    "            B+=1\n",
    "    return A, B\n",
    "\n",
    "A_score, B_score = N_simulation(pair, \"RCM\", 100)\n",
    "print(A_score, B_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Results and Analysis (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
