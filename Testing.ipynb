{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59049 pairs of rankings\n",
      "\n",
      "P:  ('N', 'R', 'N', 'HR', 'HR') \n",
      "E:  ('R', 'N', 'N', 'HR', 'R')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'R', 'R', 'R', 'N', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.267 \tE:0.350\n",
      "nDCG: \tP:0.099 \tE:0.156\n",
      "ERR: \tP:0.125 \tE:0.250\n",
      "\n",
      "P:  ('HR', 'R', 'R', 'R', 'N') \n",
      "E:  ('R', 'N', 'HR', 'N', 'HR')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'R', 'R', 'R', 'R', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.571 \tE:0.324\n",
      "nDCG: \tP:0.646 \tE:0.391\n",
      "ERR: \tP:0.797 \tE:0.438\n",
      "\n",
      "P:  ('HR', 'HR', 'R', 'N', 'HR') \n",
      "E:  ('R', 'R', 'HR', 'N', 'N')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'R', 'R', 'R', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.543 \tE:0.429\n",
      "nDCG: \tP:0.844 \tE:0.490\n",
      "ERR: \tP:0.849 \tE:0.484\n",
      "\n",
      "P:  ('HR', 'HR', 'HR', 'HR', 'R') \n",
      "E:  ('N', 'N', 'N', 'R', 'R')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'R', 'R', 'R', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.714 \tE:0.093\n",
      "nDCG: \tP:1.000 \tE:0.000\n",
      "ERR: \tP:0.859 \tE:0.000\n",
      "\n",
      "P:  ('R', 'HR', 'N', 'HR', 'N') \n",
      "E:  ('N', 'R', 'R', 'HR', 'R')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'R', 'R', 'R', 'R', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.393 \tE:0.388\n",
      "nDCG: \tP:0.453 \tE:0.177\n",
      "ERR: \tP:0.531 \tE:0.188\n",
      "\n",
      "P:  ('R', 'R', 'N', 'N', 'R') \n",
      "E:  ('HR', 'HR', 'HR', 'HR', 'N')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'R', 'R', 'R', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.371 \tE:0.571\n",
      "nDCG: \tP:0.255 \tE:1.000\n",
      "ERR: \tP:0.344 \tE:0.859\n",
      "\n",
      "P:  ('HR', 'R', 'HR', 'HR', 'N') \n",
      "E:  ('N', 'N', 'HR', 'R', 'N')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'R', 'R', 'N', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.667 \tE:0.139\n",
      "nDCG: \tP:0.803 \tE:0.235\n",
      "ERR: \tP:0.828 \tE:0.250\n",
      "\n",
      "P:  ('HR', 'HR', 'HR', 'R', 'HR') \n",
      "E:  ('HR', 'N', 'R', 'R', 'N')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'HR', 'R', 'R', 'R', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.625 \tE:0.302\n",
      "nDCG: \tP:1.000 \tE:0.547\n",
      "ERR: \tP:0.859 \tE:0.771\n",
      "\n",
      "P:  ('R', 'HR', 'N', 'R', 'R') \n",
      "E:  ('N', 'N', 'HR', 'N', 'N')\n",
      "perfect ranking:\t ['HR', 'HR', 'R', 'R', 'R', 'N', 'N', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 5.392789260714372\n",
      "AP: \tP:0.710 \tE:0.067\n",
      "nDCG: \tP:0.536 \tE:0.278\n",
      "ERR: \tP:0.531 \tE:0.250\n",
      "\n",
      "P:  ('HR', 'HR', 'HR', 'N', 'N') \n",
      "E:  ('HR', 'R', 'N', 'R', 'N')\n",
      "perfect ranking:\t ['HR', 'HR', 'HR', 'HR', 'R', 'R', 'N', 'N', 'N', 'N']\n",
      "perfect DCG score:\t 6.392789260714372\n",
      "AP: \tP:0.500 \tE:0.458\n",
      "nDCG: \tP:1.000 \tE:0.568\n",
      "ERR: \tP:0.859 \tE:0.781\n",
      "\n",
      "\n",
      "Out of 59048 rankings at k = 3, E outperformed P:\n",
      "AP: \t47.340%\n",
      "nDCG: \t47.737%\n",
      "ERR: \t47.737%\n",
      "MAP \tP:0.399 \tE:0.399\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "import math\n",
    "\n",
    "k = 3\n",
    "relevances = { 'N', 'R', 'HR' }\n",
    "relevanceScores = { 'N':0, 'R':1, 'HR':2 }\n",
    "rankingsOf5 = list(itertools.product(relevances, repeat=5))\n",
    "pairsOfRankingsOf5 = list(itertools.product(rankingsOf5, rankingsOf5))\n",
    "shuffle(pairsOfRankingsOf5)\n",
    "print(len(pairsOfRankingsOf5), \"pairs of rankings\")\n",
    "\n",
    "def getContingencies (items, k, relevantDocumentCount):\n",
    "    retrievedCounter = Counter(items[:k])\n",
    "    TP = retrievedCounter['R'] + retrievedCounter['HR']\n",
    "    FP = retrievedCounter['N']\n",
    "    \n",
    "    notRetrievedCounter = Counter(items[k:])\n",
    "    TN = notRetrievedCounter['N']\n",
    "    FN = relevantDocumentCount - TP\n",
    "    \n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def getPrecisionAtK (ranking, k):\n",
    "    TP, FP, TN, FN = getContingencies(ranking, k, relevantDocumentCount)\n",
    "    precisionAtK = TP / (TP + FP)\n",
    "#     recallAtK = TP / (TP + FN)\n",
    "#     F1AtK = 2*precisionAtK*recallAtK\n",
    "#     if F1AtK > 0.0: \n",
    "#         F1AtK /= precisionAtK + recallAtK\n",
    "#     accuracyAtK = (TP + TN)/(TP + FP + FN + TN)\n",
    "    return precisionAtK\n",
    "\n",
    "def getAveragePrecision (ranking, relevantDocumentCount):\n",
    "    precisionsForAp = []\n",
    "    for k in range(1, len(ranking)+1):\n",
    "        precisionAtK = getPrecisionAtK(ranking, k)\n",
    "\n",
    "        if ranking[k-1] == 'R' or ranking[k-1] == 'HR':\n",
    "            # save for calculating AP later\n",
    "            precisionsForAp.append(precisionAtK)\n",
    "    \n",
    "    averagePrecision = sum(precisionsForAp)/relevantDocumentCount\n",
    "    return averagePrecision\n",
    "\n",
    "def getDiscountedCumulativeGain (ranking):\n",
    "    dcg = 0.0\n",
    "    for r in range(1, len(ranking)+1):\n",
    "        relevanceAtR = relevanceScores[ranking[r-1]]\n",
    "        gain = (2 ** relevanceAtR) - 1\n",
    "        discount = math.log2(1 + r)\n",
    "        dcg += gain/discount\n",
    "    return dcg\n",
    "\n",
    "def getProbabilityOfRelevance (relevance):\n",
    "    # from paper, similar to DCG score\n",
    "    gain = (2 ** relevanceScores[relevance]) - 1\n",
    "    discount = 2 ** max(relevanceScores.values())\n",
    "    probability = gain / discount\n",
    "#     print('probability of', relevance, 'is', probability)\n",
    "    return probability\n",
    "\n",
    "def getExpectedReciprocalRank (ranking):\n",
    "    err = 0.0\n",
    "    for r in range(1, len(ranking)+1):\n",
    "        probabilityOfReachingRankR = 1.0\n",
    "        for j in range(r-1):\n",
    "            probabilityOfReachingRankR *= 1 - getProbabilityOfRelevance(ranking[j])\n",
    "        probabilityOfStoppingAtRankR = getProbabilityOfRelevance(ranking[r-1])\n",
    "        probabilityOfSatisfaction = probabilityOfReachingRankR * probabilityOfStoppingAtRankR\n",
    "        expectedProbabilityOfSatisfaction = probabilityOfSatisfaction / r\n",
    "        err += expectedProbabilityOfSatisfaction\n",
    "    return err\n",
    "\n",
    "averagePrecisionsForMapP = []\n",
    "averagePrecisionsForMapE = []\n",
    "pairCountForWhichEHasBetterAp = 0\n",
    "pairCountForWhichEHasBetterNDcg = 0\n",
    "pairCountForWhichEHasBetterErr = 0\n",
    "for i, rankingPair in enumerate(pairsOfRankingsOf5):\n",
    "    P = rankingPair[0]\n",
    "    E = rankingPair[1]\n",
    "\n",
    "    # implement 1 of (binary):\n",
    "    #   precision at rank k\n",
    "    #   recall at rank k\n",
    "    #   average precision   <--\n",
    "    totalCounter = Counter(P) + Counter(E)\n",
    "    relevantDocumentCount = totalCounter['R'] + totalCounter['HR']\n",
    "    \n",
    "    if relevantDocumentCount == 0:\n",
    "        # result is irrelevant\n",
    "        continue\n",
    "    \n",
    "    averagePrecisionP = getAveragePrecision(P, relevantDocumentCount)\n",
    "    averagePrecisionE = getAveragePrecision(E, relevantDocumentCount)\n",
    "    \n",
    "    # save for calculating MAP later\n",
    "    averagePrecisionsForMapP.append(averagePrecisionP)\n",
    "    averagePrecisionsForMapE.append(averagePrecisionE)\n",
    "\n",
    "    # implement 2 of (multi-graded):\n",
    "    #   nDCG at rank k\n",
    "    #   ERR\n",
    "    \n",
    "    # Normalized Discounted Cumulative Gain\n",
    "    # First we have to determine the perfect ranking. Assuming the P and E results are always\n",
    "    # different, and that both algorithms run on the same corpus of documents, the perfect ranking \n",
    "    # would include the results from both rankings.\n",
    "    mergedRanking = P + E\n",
    "    perfectRanking = sorted(mergedRanking, key=lambda relevance: relevanceScores[relevance], reverse=True)\n",
    "    perfectDcgScore = getDiscountedCumulativeGain(perfectRanking[:k])\n",
    "    dcgAtKP = getDiscountedCumulativeGain(P[:k])\n",
    "    dcgAtKE = getDiscountedCumulativeGain(E[:k])\n",
    "    nDcgAtKP = dcgAtKP / perfectDcgScore\n",
    "    nDcgAtKE = dcgAtKE / perfectDcgScore\n",
    "    \n",
    "    \n",
    "    # Expected Reciprocal Rank\n",
    "    errP = getExpectedReciprocalRank(P[:k])\n",
    "    errE = getExpectedReciprocalRank(E[:k])\n",
    "    \n",
    "    # calculate delta measures\n",
    "    deltaAp = averagePrecisionE - averagePrecisionP\n",
    "    deltaNDcg = nDcgAtKE - nDcgAtKP\n",
    "    deltaErr = errE - errP\n",
    "    \n",
    "    # count pairs for which E outperforms P\n",
    "    epsilon = 1e-6 # avoid floating point imprecisions\n",
    "    if deltaAp > epsilon:\n",
    "        pairCountForWhichEHasBetterAp += 1\n",
    "    if deltaNDcg > epsilon:\n",
    "        pairCountForWhichEHasBetterNDcg += 1\n",
    "    if deltaErr > epsilon:\n",
    "        pairCountForWhichEHasBetterErr += 1\n",
    "    \n",
    "    # only show a few\n",
    "    if i < 10:\n",
    "        # show the pair\n",
    "        print ('\\nP: ', P, '\\nE: ', E)\n",
    "        print('perfect ranking:\\t', perfectRanking)\n",
    "        print('perfect DCG score:\\t', perfectDcgScore)\n",
    "        print('AP: \\tP:{:.3f} \\tE:{:.3f}'.format(averagePrecisionP, averagePrecisionE))\n",
    "        print('nDCG: \\tP:{:.3f} \\tE:{:.3f}'.format(nDcgAtKP, nDcgAtKE))\n",
    "        print('ERR: \\tP:{:.3f} \\tE:{:.3f}'.format(errP, errE))\n",
    "        \n",
    "# print results\n",
    "\n",
    "# print how many times E outperformed P\n",
    "print('\\n\\nOut of {} rankings at k = {}, E outperformed P:'.format(len(pairsOfRankingsOf5)-1, k))\n",
    "print('AP: \\t{:.3%}'.format(pairCountForWhichEHasBetterAp/len(pairsOfRankingsOf5)))\n",
    "print('nDCG: \\t{:.3%}'.format(pairCountForWhichEHasBetterNDcg/len(pairsOfRankingsOf5)))\n",
    "print('ERR: \\t{:.3%}'.format(pairCountForWhichEHasBetterErr/len(pairsOfRankingsOf5)))\n",
    "        \n",
    "# we accidentally implemented MAP instead of just AP, but we'll leave it in\n",
    "meanAveragePrecisionP = sum(averagePrecisionsForMapP)/len(averagePrecisionsForMapP)\n",
    "meanAveragePrecisionE = sum(averagePrecisionsForMapE)/len(averagePrecisionsForMapE)\n",
    "print('MAP \\tP:{:.3f} \\tE:{:.3f}'.format(meanAveragePrecisionP, meanAveragePrecisionE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
